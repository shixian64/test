"""
Stream Processor

This module provides real-time stream processing for Android logs.
Handles continuous analysis, filtering, aggregation, and alerting.
"""

import threading
import queue
import logging
import time
from typing import Dict, Any, List, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import defaultdict, deque
from enum import Enum

from ..collectors.adb_collector import LogEntry

logger = logging.getLogger(__name__)


class AlertLevel(Enum):
    """Alert severity levels"""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class Alert:
    """Alert generated by stream processor"""
    id: str
    level: AlertLevel
    title: str
    description: str
    timestamp: datetime
    source: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    count: int = 1


@dataclass
class ProcessorConfig:
    """Configuration for stream processor"""
    window_size: int = 60  # seconds
    max_alerts_per_minute: int = 10
    error_threshold: int = 5  # errors per minute
    crash_threshold: int = 1  # crashes per minute
    anr_threshold: int = 1  # ANRs per minute
    memory_threshold: int = 3  # memory issues per minute
    enable_ml_analysis: bool = True
    buffer_size: int = 1000


@dataclass
class StreamMetrics:
    """Real-time stream metrics"""
    total_logs: int = 0
    logs_per_second: float = 0.0
    error_rate: float = 0.0
    crash_count: int = 0
    anr_count: int = 0
    memory_issues: int = 0
    alerts_generated: int = 0
    last_update: datetime = field(default_factory=datetime.now)


class StreamProcessor:
    """Real-time log stream processor"""
    
    def __init__(self, config: Optional[ProcessorConfig] = None):
        self.config = config or ProcessorConfig()
        self.is_running = False
        self.input_queue = queue.Queue(maxsize=self.config.buffer_size)
        self.alert_queue = queue.Queue()
        self.thread = None
        
        # Metrics and state
        self.metrics = StreamMetrics()
        self.window_logs = deque()  # Sliding window of logs
        self.alert_callbacks = []
        self.analysis_callbacks = []
        
        # Pattern counters for sliding window
        self.pattern_counters = defaultdict(lambda: deque())
        
        # Alert deduplication
        self.recent_alerts = {}  # alert_key -> last_time
        self.alert_suppression_time = 60  # seconds
        
        # ML integration
        self.ml_analyzer = None
        if self.config.enable_ml_analysis:
            self._initialize_ml()
    
    def _initialize_ml(self):
        """Initialize ML components if available"""
        try:
            from ...ml import create_ml_analyzer, ML_DEPENDENCIES_AVAILABLE
            if ML_DEPENDENCIES_AVAILABLE:
                self.ml_analyzer = create_ml_analyzer()
                logger.info("ML analyzer initialized for stream processing")
            else:
                logger.info("ML dependencies not available, using rule-based analysis")
        except ImportError:
            logger.info("ML module not available for stream processing")
    
    def add_alert_callback(self, callback: Callable[[Alert], None]):
        """Add callback for alert notifications"""
        self.alert_callbacks.append(callback)
    
    def add_analysis_callback(self, callback: Callable[[Dict[str, Any]], None]):
        """Add callback for analysis results"""
        self.analysis_callbacks.append(callback)
    
    def start(self) -> bool:
        """Start stream processing"""
        if self.is_running:
            logger.warning("Stream processor is already running")
            return False
        
        try:
            self.is_running = True
            self.thread = threading.Thread(target=self._process_stream, daemon=True)
            self.thread.start()
            
            logger.info("Stream processor started")
            return True
            
        except Exception as e:
            logger.error(f"Failed to start stream processor: {e}")
            self.is_running = False
            return False
    
    def stop(self):
        """Stop stream processing"""
        if not self.is_running:
            return
        
        self.is_running = False
        
        if self.thread and self.thread.is_alive():
            self.thread.join(timeout=5)
        
        logger.info("Stream processor stopped")
    
    def process_log(self, log_entry: LogEntry):
        """Add log entry to processing queue"""
        try:
            self.input_queue.put_nowait(log_entry)
        except queue.Full:
            # Drop oldest log if queue is full
            try:
                self.input_queue.get_nowait()
                self.input_queue.put_nowait(log_entry)
            except queue.Empty:
                pass
    
    def get_alert(self, timeout: Optional[float] = None) -> Optional[Alert]:
        """Get next alert from queue"""
        try:
            return self.alert_queue.get(timeout=timeout)
        except queue.Empty:
            return None
    
    def get_metrics(self) -> StreamMetrics:
        """Get current stream metrics"""
        return self.metrics
    
    def _process_stream(self):
        """Main stream processing loop"""
        last_metrics_update = time.time()
        
        while self.is_running:
            try:
                # Get log entry with timeout
                log_entry = self.input_queue.get(timeout=1.0)
                
                # Process the log entry
                self._process_log_entry(log_entry)
                
                # Update metrics periodically
                current_time = time.time()
                if current_time - last_metrics_update >= 1.0:  # Update every second
                    self._update_metrics()
                    last_metrics_update = current_time
                
            except queue.Empty:
                # No logs to process, continue
                continue
            except Exception as e:
                logger.error(f"Error in stream processing: {e}")
    
    def _process_log_entry(self, log_entry: LogEntry):
        """Process a single log entry"""
        current_time = datetime.now()
        
        # Add to sliding window
        self.window_logs.append((current_time, log_entry))
        self._cleanup_window(current_time)
        
        # Update basic metrics
        self.metrics.total_logs += 1
        
        # Analyze log entry
        analysis_result = self._analyze_log_entry(log_entry)
        
        # Check for patterns and generate alerts
        self._check_patterns(log_entry, current_time)
        
        # ML analysis if available
        if self.ml_analyzer and analysis_result.get('requires_ml_analysis'):
            self._perform_ml_analysis([log_entry])
        
        # Notify analysis callbacks
        for callback in self.analysis_callbacks:
            try:
                callback(analysis_result)
            except Exception as e:
                logger.error(f"Analysis callback error: {e}")
    
    def _analyze_log_entry(self, log_entry: LogEntry) -> Dict[str, Any]:
        """Analyze individual log entry"""
        analysis = {
            'timestamp': log_entry.timestamp,
            'level': log_entry.level,
            'tag': log_entry.tag,
            'is_error': log_entry.level in ['E', 'F'],
            'is_warning': log_entry.level == 'W',
            'patterns_detected': [],
            'requires_ml_analysis': False
        }
        
        message_lower = log_entry.message.lower()
        
        # Detect specific patterns
        if 'exception' in message_lower or 'fatal' in message_lower:
            analysis['patterns_detected'].append('crash')
            analysis['requires_ml_analysis'] = True
            self.metrics.crash_count += 1
        
        if 'anr' in message_lower or 'not responding' in message_lower:
            analysis['patterns_detected'].append('anr')
            self.metrics.anr_count += 1
        
        if 'memory' in message_lower or 'oom' in message_lower:
            analysis['patterns_detected'].append('memory_issue')
            self.metrics.memory_issues += 1
        
        if 'timeout' in message_lower or 'slow' in message_lower:
            analysis['patterns_detected'].append('performance_issue')
        
        return analysis
    
    def _check_patterns(self, log_entry: LogEntry, current_time: datetime):
        """Check for alert-worthy patterns"""
        # Count errors in current window
        window_start = current_time - timedelta(seconds=self.config.window_size)
        error_count = sum(1 for ts, entry in self.window_logs 
                         if ts >= window_start and entry.level in ['E', 'F'])
        
        # Check error threshold
        if error_count >= self.config.error_threshold:
            self._generate_alert(
                "high_error_rate",
                AlertLevel.ERROR,
                "High Error Rate Detected",
                f"Detected {error_count} errors in the last {self.config.window_size} seconds",
                {"error_count": error_count, "window_size": self.config.window_size}
            )
        
        # Check for crash patterns
        if 'exception' in log_entry.message.lower() or 'fatal' in log_entry.message.lower():
            self._generate_alert(
                "application_crash",
                AlertLevel.CRITICAL,
                "Application Crash Detected",
                f"Crash in {log_entry.tag}: {log_entry.message[:100]}...",
                {"tag": log_entry.tag, "pid": log_entry.pid}
            )
        
        # Check for ANR patterns
        if 'anr' in log_entry.message.lower():
            self._generate_alert(
                "anr_detected",
                AlertLevel.ERROR,
                "ANR Detected",
                f"Application Not Responding: {log_entry.message[:100]}...",
                {"tag": log_entry.tag, "pid": log_entry.pid}
            )
        
        # Check for memory issues
        if 'outofmemoryerror' in log_entry.message.lower():
            self._generate_alert(
                "out_of_memory",
                AlertLevel.ERROR,
                "Out of Memory Error",
                f"OOM in {log_entry.tag}: {log_entry.message[:100]}...",
                {"tag": log_entry.tag, "pid": log_entry.pid}
            )
    
    def _generate_alert(self, alert_key: str, level: AlertLevel, title: str, 
                       description: str, metadata: Dict[str, Any]):
        """Generate and queue an alert"""
        current_time = datetime.now()
        
        # Check for alert suppression
        if alert_key in self.recent_alerts:
            last_time = self.recent_alerts[alert_key]
            if (current_time - last_time).total_seconds() < self.alert_suppression_time:
                return  # Suppress duplicate alert
        
        # Create alert
        alert = Alert(
            id=f"{alert_key}_{int(current_time.timestamp())}",
            level=level,
            title=title,
            description=description,
            timestamp=current_time,
            source="stream_processor",
            metadata=metadata
        )
        
        # Update suppression tracking
        self.recent_alerts[alert_key] = current_time
        
        # Queue alert
        try:
            self.alert_queue.put_nowait(alert)
            self.metrics.alerts_generated += 1
            
            # Notify callbacks
            for callback in self.alert_callbacks:
                try:
                    callback(alert)
                except Exception as e:
                    logger.error(f"Alert callback error: {e}")
                    
        except queue.Full:
            logger.warning("Alert queue is full, dropping alert")
    
    def _perform_ml_analysis(self, log_entries: List[LogEntry]):
        """Perform ML analysis on log entries"""
        if not self.ml_analyzer:
            return
        
        try:
            # Convert log entries to text
            log_lines = [entry.raw_line for entry in log_entries]
            
            # Crash classification
            if 'crash_classifier' in self.ml_analyzer:
                classifier = self.ml_analyzer['crash_classifier']
                for log_line in log_lines:
                    if 'exception' in log_line.lower() or 'fatal' in log_line.lower():
                        prediction = classifier.classify_crash(log_line)
                        
                        if prediction.confidence > 0.8:  # High confidence
                            self._generate_alert(
                                f"ml_crash_{prediction.crash_type}",
                                AlertLevel.CRITICAL if prediction.severity == 'critical' else AlertLevel.ERROR,
                                f"ML Detected: {prediction.crash_type.title()} Crash",
                                f"{prediction.description} (confidence: {prediction.confidence:.2f})",
                                {
                                    "crash_type": prediction.crash_type,
                                    "confidence": prediction.confidence,
                                    "ml_recommendations": prediction.recommendations
                                }
                            )
            
            # Anomaly detection
            if 'anomaly_detector' in self.ml_analyzer:
                detector = self.ml_analyzer['anomaly_detector']
                anomalies = detector.detect_anomalies(log_lines)
                
                for anomaly in anomalies:
                    if anomaly.is_anomaly and anomaly.anomaly_score > 0.7:
                        alert_level = AlertLevel.CRITICAL if anomaly.severity == 'critical' else AlertLevel.WARNING
                        
                        self._generate_alert(
                            f"ml_anomaly_{anomaly.anomaly_type}",
                            alert_level,
                            f"ML Anomaly: {anomaly.anomaly_type.title()}",
                            anomaly.description,
                            {
                                "anomaly_type": anomaly.anomaly_type,
                                "score": anomaly.anomaly_score,
                                "ml_recommendations": anomaly.recommendations
                            }
                        )
            
        except Exception as e:
            logger.error(f"ML analysis error: {e}")
    
    def _cleanup_window(self, current_time: datetime):
        """Remove old entries from sliding window"""
        window_start = current_time - timedelta(seconds=self.config.window_size)
        
        while self.window_logs and self.window_logs[0][0] < window_start:
            self.window_logs.popleft()
    
    def _update_metrics(self):
        """Update stream metrics"""
        current_time = datetime.now()
        
        # Calculate logs per second
        if self.metrics.last_update:
            time_diff = (current_time - self.metrics.last_update).total_seconds()
            if time_diff > 0:
                # This is an approximation - in a real implementation,
                # you'd track logs in the last second more precisely
                recent_logs = len([ts for ts, _ in self.window_logs 
                                 if (current_time - ts).total_seconds() <= 1.0])
                self.metrics.logs_per_second = recent_logs
        
        # Calculate error rate
        if self.window_logs:
            error_logs = sum(1 for _, entry in self.window_logs if entry.level in ['E', 'F'])
            self.metrics.error_rate = error_logs / len(self.window_logs)
        
        self.metrics.last_update = current_time
